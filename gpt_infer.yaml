inference:
  greedy: False # Whether or not to use sampling ; use greedy decoding otherwise
  top_k: 1  # The number of highest probability vocabulary tokens to keep for top-k-filtering.
  top_p: 1 # If set to float < 1, only the most probable tokens with probabilities that add up to top_p or higher are kept for generation.
  temperature: 0.5 # sampling temperature
  add_BOS: True # add the bos token at the begining of the prompt
  tokens_to_generate: 500 # The minimum length of the sequence to be generated.
  all_probs: False  # whether return the log prob for all the tokens in vocab
  repetition_penalty: 1.0  # The parameter for repetition penalty. 1.0 means no penalty.
  min_tokens_to_generate: 1  # The minimum length of the sequence to be generated.
  compute_logprob: False  # a flag used to compute logprob of all the input text, a very special case of running inference, default False

trainer:
  devices: 1
  num_nodes: 1
  accelerator: gpu
  logger: False # logger provided by exp_manager
  precision: 16 # 16, 32, or bf16

model_path: /models/gpt2b